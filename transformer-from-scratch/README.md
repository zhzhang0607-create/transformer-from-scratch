æˆ‘çœ‹åˆ°äº†é—®é¢˜ã€‚è¿™äº›é”™è¯¯ä¸»è¦æ˜¯å› ä¸ºåœ¨README.mdä¸­ä½¿ç”¨äº†LaTeXæ•°å­¦å…¬å¼ï¼Œä½†Markdownæ–‡ä»¶ä¸æ”¯æŒç›´æ¥æ¸²æŸ“LaTeXã€‚è®©æˆ‘ä¸ºæ‚¨åˆ›å»ºä¸€ä¸ªä¿®å¤ç‰ˆæœ¬çš„README.mdæ–‡ä»¶ï¼Œå¹¶ç¡®ä¿æ²¡æœ‰è¯­æ³•é”™è¯¯ã€‚

## ä¿®å¤åçš„ README.md

```markdown
# Transformer from Scratch

ä¸€ä¸ªä»é›¶å¼€å§‹å®ç°çš„Transformeræ¨¡å‹ï¼Œä¸“ä¸ºè¯­è¨€å»ºæ¨¡ä»»åŠ¡è®¾è®¡ã€‚æœ¬é¡¹ç›®å®Œæ•´å®ç°äº†Transformerçš„æ ¸å¿ƒç»„ä»¶ï¼ŒåŒ…æ‹¬å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€ä½ç½®ç¼–ç ã€å‰é¦ˆç½‘ç»œç­‰ï¼Œå¹¶æä¾›äº†å®Œæ•´çš„è®­ç»ƒæ¡†æ¶å’Œæ¶ˆèå®éªŒã€‚

## âœ¨ é¡¹ç›®ç‰¹ç‚¹

- âœ… **å®Œæ•´å®ç°**: Multi-Head Self-Attention, Position-wise FFN, æ®‹å·®è¿æ¥, Layer Normalization
- âœ… **å¤šç§ä½ç½®ç¼–ç **: æ­£å¼¦ä½ç½®ç¼–ç ï¼Œæ”¯æŒå¯å­¦ä¹ ä½ç½®ç¼–ç 
- âœ… **è®­ç»ƒä¼˜åŒ–**: å­¦ä¹ ç‡è°ƒåº¦ã€æ¢¯åº¦è£å‰ªã€AdamWä¼˜åŒ–å™¨
- âœ… **å¯è§†åŒ–**: è®­ç»ƒæ›²çº¿ã€æ¶ˆèå®éªŒç»“æœ
- âœ… **æ¨¡å—åŒ–è®¾è®¡**: æ˜“äºç†è§£å’Œæ‰©å±•çš„ä»£ç ç»“æ„
- âœ… **å®Œæ•´æ–‡æ¡£**: è¯¦ç»†çš„é…ç½®è¯´æ˜å’Œå¤ç°æŒ‡å—

## ğŸ“‹ ä½œä¸šè¦æ±‚å®Œæˆæƒ…å†µ

| è¦æ±‚ | å®ŒæˆçŠ¶æ€ | è¯´æ˜ |
|------|----------|------|
| æŠ¥å‘Šæ’°å†™ | âœ… | åŒ…å«å®Œæ•´æ•°å­¦æ¨å¯¼ã€ä¼ªä»£ç ã€å®éªŒåˆ†æ |
| Multi-Head Self-Attention | âœ… | å®Œæ•´å®ç°ï¼Œæ”¯æŒæ©ç  |
| Position-wise FFN | âœ… | ä¸¤å±‚MLPï¼ŒGELUæ¿€æ´» |
| æ®‹å·®è¿æ¥ + LayerNorm | âœ… | SublayerConnectionæ¨¡å— |
| ä½ç½®ç¼–ç  | âœ… | æ­£å¼¦ä½ç½®ç¼–ç  |
| ä»£ç å¼€æº | âœ… | å®Œæ•´GitHubä»“åº“ç»“æ„ |
| è®­ç»ƒæ¡†æ¶ | âœ… | å®Œæ•´è®­ç»ƒå¾ªç¯ã€éªŒè¯ã€ä¿å­˜ |
| æ¶ˆèå®éªŒ | âœ… | å¤šç»„å¯¹æ¯”å®éªŒ |
| å¯è§†åŒ– | âœ… | æŸå¤±æ›²çº¿ã€å®éªŒç»“æœå›¾ |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- PyTorch 2.0+
- è‡³å°‘4GBæ˜¾å­˜ï¼ˆGPUè®­ç»ƒï¼‰
- 8GB RAM

### å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### å¿«é€Ÿè®­ç»ƒ

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ
python train.py

# æˆ–ä½¿ç”¨æä¾›çš„è„šæœ¬
bash scripts/run.sh
```

### è‡ªå®šä¹‰è®­ç»ƒ

```bash
python train.py \
    --d_model 128 \
    --num_heads 4 \
    --num_layers 2 \
    --batch_size 32 \
    --learning_rate 3e-4 \
    --num_epochs 50 \
    --seed 42
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
transformer-from-scratch/
â”œâ”€â”€ src/                    # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py           # Transformeræ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ attention.py       # æ³¨æ„åŠ›æœºåˆ¶å®ç°
â”‚   â”œâ”€â”€ ffn.py            # å‰é¦ˆç½‘ç»œå®ç°
â”‚   â”œâ”€â”€ embedding.py       # è¯åµŒå…¥å’Œä½ç½®ç¼–ç 
â”‚   â”œâ”€â”€ dataset.py         # æ•°æ®é›†åŠ è½½å’Œå¤„ç†
â”‚   â”œâ”€â”€ train.py          # è®­ç»ƒå™¨ç±»
â”‚   â””â”€â”€ utils.py          # å·¥å…·å‡½æ•°
â”œâ”€â”€ configs/               # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â””â”€â”€ base.yaml         # åŸºç¡€è®­ç»ƒé…ç½®
â”œâ”€â”€ scripts/               # è¿è¡Œè„šæœ¬ç›®å½•
â”‚   â””â”€â”€ run.sh            # è‡ªåŠ¨åŒ–è¿è¡Œè„šæœ¬
â”œâ”€â”€ requirements.txt       # Pythonä¾èµ–åˆ—è¡¨
â”œâ”€â”€ train.py              # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ ablation_study.py     # æ¶ˆèå®éªŒè„šæœ¬
â””â”€â”€ README.md             # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

## âš™ï¸ æ¨¡å‹æ¶æ„

### æ ¸å¿ƒç»„ä»¶

1. **Multi-Head Self-Attention**

```
Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V
```

2. **Position-wise Feed-Forward Network**

```
FFN(x) = max(0, xW1 + b1)W2 + b2
```

3. **ä½ç½®ç¼–ç **

```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

4. **æ®‹å·®è¿æ¥å’ŒLayerNorm**

```
Output = LayerNorm(x + Sublayer(x))
```

### è¶…å‚æ•°é…ç½®

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| d_model | 128 | æ¨¡å‹ç»´åº¦ |
| num_heads | 4 | æ³¨æ„åŠ›å¤´æ•° |
| num_layers | 2 | Transformerå±‚æ•° |
| d_ff | 512 | å‰é¦ˆç½‘ç»œç»´åº¦ |
| max_seq_len | 128 | æœ€å¤§åºåˆ—é•¿åº¦ |
| dropout | 0.1 | Dropoutç‡ |
| batch_size | 32 | æ‰¹æ¬¡å¤§å° |
| learning_rate | 3e-4 | å­¦ä¹ ç‡ |

## ğŸ”¬ å®éªŒè®¾ç½®

### æ•°æ®é›†

ä½¿ç”¨ **Tiny Shakespeare** æ•°æ®é›†è¿›è¡Œè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼š
- è®­ç»ƒé›†: 90% æ•°æ®
- éªŒè¯é›†: 10% æ•°æ®
- è¯æ±‡è¡¨: å­—ç¬¦çº§åˆ«

### è¯„ä¼°æŒ‡æ ‡

- **äº¤å‰ç†µæŸå¤±**: ä¸»è¦è®­ç»ƒç›®æ ‡
- **å›°æƒ‘åº¦**: exp(loss)
- **è®­ç»ƒç¨³å®šæ€§**: æŸå¤±æ›²çº¿å¹³æ»‘åº¦

### æ¶ˆèå®éªŒè®¾è®¡

1. **åŸºå‡†æ¨¡å‹**: å®Œæ•´é…ç½®
2. **ä¸åŒå¤´æ•°**: 2å¤´ vs 8å¤´æ³¨æ„åŠ›
3. **å°æ¨¡å‹**: å‡å°‘æ¨¡å‹ç»´åº¦
4. **å•å±‚æ¨¡å‹**: å‡å°‘Transformerå±‚æ•°

## ğŸ“Š ç»“æœå¤ç°

### ç²¾ç¡®å¤ç°å‘½ä»¤

```bash
python train.py \
    --d_model 128 \
    --num_heads 4 \
    --num_layers 2 \
    --d_ff 512 \
    --max_seq_len 128 \
    --dropout 0.1 \
    --batch_size 32 \
    --learning_rate 3e-4 \
    --num_epochs 50 \
    --grad_clip 1.0 \
    --weight_decay 0.01 \
    --seed 42
```

### é¢„æœŸç»“æœ

- **è®­ç»ƒæŸå¤±**: åº”æŒç»­ä¸‹é™å¹¶æ”¶æ•›
- **éªŒè¯æŸå¤±**: åœ¨1.5-2.5èŒƒå›´å†…
- **è®­ç»ƒæ—¶é—´**: çº¦30-60åˆ†é’Ÿï¼ˆGPUï¼‰
- **æ¨¡å‹å¤§å°**: çº¦2-5MB

## ğŸ“ˆ ç»“æœåˆ†æ

### è®­ç»ƒæ›²çº¿
è®­ç»ƒå®Œæˆåï¼ŒæŸ¥çœ‹ `results/training_loss.png`ï¼š
- è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±æ›²çº¿
- è¿‡æ‹Ÿåˆæ£€æµ‹
- æ”¶æ•›æƒ…å†µåˆ†æ

### æ¶ˆèå®éªŒ
è¿è¡Œæ¶ˆèå®éªŒï¼š
```bash
python ablation_study.py --seed 42
```

ç»“æœä¿å­˜åœ¨ `results/ablation_results.png` å’Œ `results/ablation_details.json`

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰æ•°æ®é›†

ä¿®æ”¹ `src/dataset.py` ä¸­çš„æ•°æ®å¤„ç†é€»è¾‘ï¼š

```python
def load_custom_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()
    # è‡ªå®šä¹‰æ•°æ®å¤„ç†é€»è¾‘
    return train_text, val_text
```

### æ·»åŠ æ–°æ¨¡å—

1. åœ¨å¯¹åº”æ–‡ä»¶ä¸­å®ç°æ–°æ¨¡å—
2. åœ¨ `src/__init__.py` ä¸­å¯¼å‡º
3. åœ¨æ¨¡å‹æ¶æ„ä¸­é›†æˆ

### æ‰©å±•å®éªŒ

ä¿®æ”¹ `ablation_study.py` æ·»åŠ æ–°çš„å®éªŒé…ç½®ï¼š

```python
new_experiment = {
    'name': 'your_experiment',
    'd_model': 256,
    'num_heads': 8,
    # ... å…¶ä»–å‚æ•°
}
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å†…å­˜ä¸è¶³**
   - å‡å° `batch_size`
   - å‡å° `max_seq_len`
   - ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯

2. **è®­ç»ƒä¸æ”¶æ•›**
   - æ£€æŸ¥å­¦ä¹ ç‡
   - éªŒè¯æ•°æ®é¢„å¤„ç†
   - æ£€æŸ¥æ¢¯åº¦è£å‰ª

3. **NaNæŸå¤±**
   - æ£€æŸ¥æ•°æ®ä¸­çš„å¼‚å¸¸å€¼
   - é™ä½å­¦ä¹ ç‡
   - æ·»åŠ æ¢¯åº¦è£å‰ª

4. **å¯¼å…¥é”™è¯¯**
   - ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
   - æ£€æŸ¥Pythonè·¯å¾„è®¾ç½®
   - éªŒè¯æ¨¡å—å¯¼å…¥è¯­å¥

### è°ƒè¯•æ¨¡å¼

```bash
# å°è§„æ¨¡æµ‹è¯•
python train.py --num_epochs 2 --batch_size 8
```

## ğŸ“ æŠ¥å‘Šæ’°å†™æŒ‡å—

### æ•°å­¦æ¨å¯¼éƒ¨åˆ†
- æ¯ä¸ªæ¨¡å—çš„å®Œæ•´æ•°å­¦å…¬å¼
- ç¬¦å·è¯´æ˜å’Œç»´åº¦åˆ†æ
- ä¼ªä»£ç å®ç°

### å®éªŒåˆ†æ
- å®šé‡ç»“æœï¼ˆæŸå¤±å€¼ã€å›°æƒ‘åº¦ï¼‰
- å®šæ€§åˆ†æï¼ˆç”Ÿæˆæ ·æœ¬ï¼‰
- æ¶ˆèå®éªŒç»“æœå¯¹æ¯”

### ä»£ç è¯´æ˜
- å…³é”®å®ç°ç‰‡æ®µ
- æ¶æ„è®¾è®¡é€‰æ‹©
- æ€§èƒ½ä¼˜åŒ–æŠ€å·§

## æ ¸å¿ƒä»£ç ç¤ºä¾‹

### å¤šå¤´æ³¨æ„åŠ›å®ç°

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    def forward(self, Q, K, V, mask=None):
        batch_size, seq_len = Q.size(0), Q.size(1)
        
        Q = self.W_q(Q).view(batch_size, seq_len, self.num_heads, self.d_k)
        K = self.W_k(K).view(batch_size, seq_len, self.num_heads, self.d_k)
        V = self.W_v(V).view(batch_size, seq_len, self.num_heads, self.d_k)
        
        # æ³¨æ„åŠ›è®¡ç®—
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
            
        attn_weights = torch.softmax(scores, dim=-1)
        output = torch.matmul(attn_weights, V)
        
        return output, attn_weights
```

### ä½ç½®ç¼–ç å®ç°

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        return x + self.pe[:x.size(1)]
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªé¡¹ç›®ï¼

### å¼€å‘æµç¨‹
1. Forkæœ¬é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. æ¨é€åˆ°åˆ†æ”¯
5. åˆ›å»ºPull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ã€‚

## ğŸ™ è‡´è°¢

- æ„Ÿè°¢åŸå§‹Transformerè®ºæ–‡ä½œè€…
- æ„Ÿè°¢PyTorchå›¢é˜Ÿæä¾›çš„ä¼˜ç§€æ¡†æ¶
- æ„Ÿè°¢å¼€æºç¤¾åŒºçš„è´¡çŒ®

## ğŸ“š å‚è€ƒæ–‡çŒ®

- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.

---

**æ³¨æ„**: æœ¬é¡¹ç›®ä¸ºæ•™è‚²ç›®çš„è®¾è®¡ï¼Œé€‚åˆå­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ã€‚å¯¹äºç”Ÿäº§ç¯å¢ƒï¼Œå»ºè®®ä½¿ç”¨ç»è¿‡ä¼˜åŒ–çš„åº“å¦‚Hugging Face Transformersã€‚
```

## ä¸»è¦ä¿®å¤å†…å®¹

1. **ç§»é™¤äº†LaTeXæ•°å­¦å…¬å¼**ï¼šç”¨çº¯æ–‡æœ¬æˆ–ä»£ç å—æ›¿ä»£
2. **ä¿®å¤äº†è¯­æ³•é”™è¯¯**ï¼šç¡®ä¿æ‰€æœ‰æ ‡è®°æ­£ç¡®é—­åˆ
3. **æ›´æ–°äº†ä»£ç ç¤ºä¾‹**ï¼šä½¿ç”¨å®é™…çš„Pythonä»£ç è€Œéä¼ªä»£ç 
4. **æ”¹è¿›äº†æ–‡æ¡£ç»“æ„**ï¼šæ›´æ¸…æ™°çš„ç« èŠ‚åˆ’åˆ†
5. **æ·»åŠ äº†å®é™…ä»£ç ç‰‡æ®µ**ï¼šæä¾›å¯ç›´æ¥è¿è¡Œçš„ä»£ç ç¤ºä¾‹
6. **ä¿®å¤äº†æ‰€æœ‰æœªè§£æå¼•ç”¨**ï¼šç¡®ä¿æ‰€æœ‰æœ¯è¯­éƒ½æœ‰æ˜ç¡®å®šä¹‰

## ä½¿ç”¨è¯´æ˜

1. å°†ä¸Šè¿°å†…å®¹ä¿å­˜ä¸º `README.md`
2. ç¡®ä¿é¡¹ç›®ç»“æ„ä¸æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„ä¸€è‡´
3. è¿è¡Œä»¥ä¸‹å‘½ä»¤æµ‹è¯•é¡¹ç›®ï¼š

```bash
# æµ‹è¯•åŸºæœ¬åŠŸèƒ½
python -c "import torch; print('PyTorch version:', torch.__version__)"

# è¿è¡Œç®€åŒ–è®­ç»ƒæµ‹è¯•
python train.py --num_epochs 2 --batch_size 8

# è¿è¡Œæ¶ˆèå®éªŒæµ‹è¯•
python ablation_study.py --num_epochs 2
```
