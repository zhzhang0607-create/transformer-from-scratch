# Transformer from Scratch

ä¸€ä¸ªä»é›¶å¼€å§‹å®ç°çš„Transformeræ¨¡å‹ï¼Œä¸“ä¸ºè¯­è¨€å»ºæ¨¡ä»»åŠ¡è®¾è®¡ã€‚æœ¬é¡¹ç›®å®Œæ•´å®ç°äº†Transformerç¼–ç å™¨æ¶æ„ï¼ŒåŒ…å«å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€ä½ç½®ç¼–ç ã€å‰é¦ˆç½‘ç»œç­‰æ ¸å¿ƒç»„ä»¶ï¼Œå¹¶æä¾›äº†å®Œæ•´çš„è®­ç»ƒæ¡†æ¶å’Œæ¶ˆèå®éªŒã€‚

## âœ¨ é¡¹ç›®ç‰¹ç‚¹

- âœ… **å®Œæ•´å®ç°**: Multi-Head Self-Attention, Position-wise FFN, æ®‹å·®è¿æ¥, Layer Normalization
- âœ… **å¤šç§ä½ç½®ç¼–ç **: æ­£å¼¦ä½ç½®ç¼–ç 
- âœ… **è®­ç»ƒä¼˜åŒ–**: å­¦ä¹ ç‡è°ƒåº¦ã€æ¢¯åº¦è£å‰ªã€AdamWä¼˜åŒ–å™¨
- âœ… **å¯è§†åŒ–**: è®­ç»ƒæ›²çº¿ã€æ¶ˆèå®éªŒç»“æœ
- âœ… **æ¨¡å—åŒ–è®¾è®¡**: æ˜“äºç†è§£å’Œæ‰©å±•çš„ä»£ç ç»“æ„
- âœ… **å®Œæ•´æ–‡æ¡£**: è¯¦ç»†çš„é…ç½®è¯´æ˜å’Œå¤ç°æŒ‡å—

## ğŸ“‹ ä½œä¸šè¦æ±‚å®Œæˆæƒ…å†µ

| è¦æ±‚ | å®ŒæˆçŠ¶æ€ | è¯´æ˜ |
|------|----------|------|
| æŠ¥å‘Šæ’°å†™ | âœ… | åŒ…å«å®Œæ•´æ•°å­¦æ¨å¯¼ã€ä¼ªä»£ç ã€å®éªŒåˆ†æ |
| Multi-Head Self-Attention | âœ… | å®Œæ•´å®ç°ï¼Œæ”¯æŒæ©ç  |
| Position-wise FFN | âœ… | ä¸¤å±‚MLPï¼ŒGELUæ¿€æ´» |
| æ®‹å·®è¿æ¥ + LayerNorm | âœ… | SublayerConnectionæ¨¡å— |
| ä½ç½®ç¼–ç  | âœ… | æ­£å¼¦ä½ç½®ç¼–ç  |
| ä»£ç å¼€æº | âœ… | å®Œæ•´GitHubä»“åº“ç»“æ„ |
| è®­ç»ƒæ¡†æ¶ | âœ… | å®Œæ•´è®­ç»ƒå¾ªç¯ã€éªŒè¯ã€ä¿å­˜ |
| æ¶ˆèå®éªŒ | âœ… | 5ç»„å¯¹æ¯”å®éªŒ |
| å¯è§†åŒ– | âœ… | æŸå¤±æ›²çº¿ã€å®éªŒç»“æœå›¾ |
| Encoderå®ç° | âœ… | å®Œæ•´Transformerç¼–ç å™¨æ¶æ„ |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- **Python**: 3.8+
- **PyTorch**: 2.0+
- **å†…å­˜**: è‡³å°‘8GB RAM
- **å­˜å‚¨**: è‡³å°‘2GBå¯ç”¨ç©ºé—´
- **GPU** (å¯é€‰): 4GB+ VRAMç”¨äºåŠ é€Ÿè®­ç»ƒ

### å®‰è£…ä¾èµ–

```bash
# åˆ›å»ºcondaç¯å¢ƒï¼ˆæ¨èï¼‰
conda create -n transformer python=3.10
conda activate transformer

# å®‰è£…PyTorchï¼ˆæ ¹æ®æ‚¨çš„CUDAç‰ˆæœ¬é€‰æ‹©ï¼‰
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# æˆ–CPUç‰ˆæœ¬
pip install torch torchvision torchaudio

# å®‰è£…é¡¹ç›®ä¾èµ–
pip install -r requirements.txt
```

### å¿«é€Ÿè®­ç»ƒ

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒï¼ˆ5ä¸ªepochå¿«é€Ÿæµ‹è¯•ï¼‰
python train.py --num_epochs 5

# å®Œæ•´è®­ç»ƒï¼ˆ10ä¸ªepochï¼‰
python train.py --seed 42
```

### æ¶ˆèå®éªŒ

```bash
# è¿è¡Œå®Œæ•´æ¶ˆèå®éªŒï¼ˆ5ä¸ªé…ç½®ï¼Œå„5ä¸ªepochï¼‰
python ablation_study.py --seed 42

# å¿«é€Ÿæµ‹è¯•ç‰ˆæœ¬
python ablation_study.py --num_epochs 2 --seed 42
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
transformer-from-scratch/
â”œâ”€â”€ src/                    # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py           # Transformerç¼–ç å™¨æ¨¡å‹
â”‚   â”œâ”€â”€ attention.py       # æ³¨æ„åŠ›æœºåˆ¶å®ç°
â”‚   â”œâ”€â”€ ffn.py            # å‰é¦ˆç½‘ç»œå®ç°
â”‚   â”œâ”€â”€ embedding.py       # è¯åµŒå…¥å’Œä½ç½®ç¼–ç 
â”‚   â”œâ”€â”€ dataset.py         # æ•°æ®é›†åŠ è½½å’Œå¤„ç†
â”‚   â””â”€â”€ utils.py          # å·¥å…·å‡½æ•°
â”œâ”€â”€ configs/               # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â””â”€â”€ base.yaml         # åŸºç¡€è®­ç»ƒé…ç½®
â”œâ”€â”€ scripts/               # è¿è¡Œè„šæœ¬ç›®å½•
â”‚   â””â”€â”€ run.sh            # è‡ªåŠ¨åŒ–è¿è¡Œè„šæœ¬
â”œâ”€â”€ results/               # å®éªŒç»“æœç›®å½•ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
â”‚   â”œâ”€â”€ training_loss.png
â”‚   â”œâ”€â”€ ablation_results.png
â”‚   â””â”€â”€ ablation_details.json
â”œâ”€â”€ requirements.txt       # Pythonä¾èµ–åˆ—è¡¨
â”œâ”€â”€ train.py              # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ ablation_study.py     # æ¶ˆèå®éªŒè„šæœ¬
â”œâ”€â”€ README.md             # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â””â”€â”€ .gitignore           # Gitå¿½ç•¥æ–‡ä»¶
```

## âš™ï¸ æ¨¡å‹æ¶æ„

### æ ¸å¿ƒç»„ä»¶

**Multi-Head Self-Attention**
```python
Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V
```

**Position-wise Feed-Forward Network**
```python
FFN(x) = GELU(xW1 + b1)W2 + b2
```

**ä½ç½®ç¼–ç **
```python
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**æ®‹å·®è¿æ¥å’ŒLayerNorm**
```python
Output = LayerNorm(x + Sublayer(x))
```

### é»˜è®¤è¶…å‚æ•°é…ç½®

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| d_model | 128 | æ¨¡å‹ç»´åº¦ |
| num_heads | 4 | æ³¨æ„åŠ›å¤´æ•° |
| num_layers | 2 | Transformerå±‚æ•° |
| d_ff | 512 | å‰é¦ˆç½‘ç»œç»´åº¦ |
| max_seq_len | 128 | æœ€å¤§åºåˆ—é•¿åº¦ |
| dropout | 0.1 | Dropoutç‡ |
| batch_size | 32 | æ‰¹æ¬¡å¤§å° |
| learning_rate | 3e-4 | å­¦ä¹ ç‡ |

## ğŸ”¬ å®éªŒè®¾ç½®

### æ•°æ®é›†

ä½¿ç”¨ **Tiny Shakespeare** æ•°æ®é›†è¿›è¡Œå­—ç¬¦çº§è¯­è¨€å»ºæ¨¡ï¼š
- **è®­ç»ƒé›†**: 1,003,854å­—ç¬¦ (90%)
- **éªŒè¯é›†**: 111,540å­—ç¬¦ (10%) 
- **è¯æ±‡è¡¨å¤§å°**: 69ä¸ªå­—ç¬¦
- **è‡ªåŠ¨ä¸‹è½½**: ä»£ç åŒ…å«æ•°æ®é›†ä¸‹è½½åŠŸèƒ½

### è¯„ä¼°æŒ‡æ ‡

- **äº¤å‰ç†µæŸå¤±**: ä¸»è¦è®­ç»ƒç›®æ ‡
- **å›°æƒ‘åº¦**: exp(loss)
- **è®­ç»ƒç¨³å®šæ€§**: æŸå¤±æ›²çº¿å¹³æ»‘åº¦

### æ¶ˆèå®éªŒè®¾è®¡

1. **baseline**: æ ‡å‡†é…ç½® (4å¤´, 2å±‚, 128ç»´)
2. **2_heads**: å‡å°‘æ³¨æ„åŠ›å¤´æ•°è‡³2
3. **8_heads**: å¢åŠ æ³¨æ„åŠ›å¤´æ•°è‡³8  
4. **small_model**: å‡å°æ¨¡å‹è§„æ¨¡ (64ç»´, 256 FFN)
5. **single_layer**: å‡å°‘ç¼–ç å™¨å±‚æ•°è‡³1

## ğŸ“Š ç²¾ç¡®å¤ç°

### å®Œæ•´è®­ç»ƒå‘½ä»¤

```bash
python train.py \
    --d_model 128 \
    --num_heads 4 \
    --num_layers 2 \
    --d_ff 512 \
    --max_seq_len 128 \
    --dropout 0.1 \
    --batch_size 32 \
    --learning_rate 3e-4 \
    --num_epochs 10 \
    --grad_clip 1.0 \
    --weight_decay 0.01 \
    --seed 42
```

### é¢„æœŸç»“æœ

- **è®­ç»ƒæ—¶é—´**: CPUçº¦2-4å°æ—¶ï¼ŒGPUçº¦30-60åˆ†é’Ÿ
- **æœ€ç»ˆéªŒè¯æŸå¤±**: çº¦2.45-2.50
- **æ¨¡å‹å¤§å°**: çº¦1.6MB (414,533å‚æ•°)
- **è®­ç»ƒæ›²çº¿**: æŒç»­æ”¶æ•›ï¼Œæ— è¿‡æ‹Ÿåˆ

## ğŸ“ˆ å®éªŒç»“æœ

### æ¶ˆèå®éªŒç»“æœ

| æ¨¡å‹é…ç½® | å‚æ•°é‡ | æœ€ç»ˆéªŒè¯æŸå¤± | ç›¸å¯¹åŸºçº¿å˜åŒ– |
|----------|--------|--------------|--------------|
| baseline | 414,533 | 2.4773 | - |
| 2_heads | 414,533 | 2.4771 | -0.0002 |
| 8_heads | 414,533 | **2.4682** | **-0.0091** |
| small_model | 108,997 | 2.5042 | +0.0269 |
| single_layer | 216,261 | 2.4837 | +0.0064 |

### ç»“æœåˆ†æ

- **8å¤´æ³¨æ„åŠ›è¡¨ç°æœ€ä½³**ï¼ŒéªŒè¯æŸå¤±2.4682
- **æ¨¡å‹å®¹é‡å¾ˆé‡è¦**ï¼Œsmall_modelæ€§èƒ½æ˜æ˜¾ä¸‹é™
- **å¤šå±‚ç»“æ„æœ‰å¸®åŠ©**ï¼Œsingle_layeræ€§èƒ½ç•¥å·®äºbaseline

## ğŸ”§ æ ¸å¿ƒä»£ç 

### å¤šå¤´æ³¨æ„åŠ›å®ç°

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        self.attention = ScaledDotProductAttention(dropout)

    def forward(self, Q, K, V, mask=None):
        batch_size, seq_len = Q.size(0), Q.size(1)
        
        # çº¿æ€§å˜æ¢å¹¶åˆ†å¤´
        Q = self.W_q(Q).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(K).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(V).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        
        # åº”ç”¨æ³¨æ„åŠ›
        attn_output, attn_weights = self.attention(Q, K, V, mask)
        
        # åˆå¹¶å¤šå¤´
        attn_output = attn_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model)
        
        return self.W_o(attn_output), attn_weights
```

### ä½ç½®ç¼–ç å®ç°

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:x.size(1), :].transpose(0, 1)
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**å†…å­˜ä¸è¶³**
```bash
# å‡å°æ‰¹æ¬¡å¤§å°
python train.py --batch_size 16

# å‡å°åºåˆ—é•¿åº¦
python train.py --max_seq_len 64
```

**è®­ç»ƒä¸æ”¶æ•›**
```bash
# è°ƒæ•´å­¦ä¹ ç‡
python train.py --learning_rate 1e-4

# æ£€æŸ¥æ¢¯åº¦
python train.py --grad_clip 0.5
```

**å¯¼å…¥é”™è¯¯**
```bash
# ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
cd transformer-from-scratch

# æ£€æŸ¥Pythonè·¯å¾„
python -c "import src; print('å¯¼å…¥æˆåŠŸ')"
```

### è°ƒè¯•æ¨¡å¼

```bash
# å¿«é€ŸåŠŸèƒ½æµ‹è¯•
python train.py --num_epochs 1 --batch_size 8
python ablation_study.py --num_epochs 1
```

## ğŸ”® æ‰©å±•å¼€å‘

### æ·»åŠ æ–°æ¨¡å—

1. åœ¨å¯¹åº”æ–‡ä»¶ä¸­å®ç°æ–°æ¨¡å—
2. åœ¨ `src/__init__.py` ä¸­å¯¼å‡º
3. åœ¨æ¨¡å‹æ¶æ„ä¸­é›†æˆ

### è‡ªå®šä¹‰æ•°æ®é›†

ä¿®æ”¹ `src/dataset.py`ï¼š

```python
def load_custom_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()
    # è‡ªå®šä¹‰æ•°æ®å¤„ç†
    return train_text, val_text
```

### æ‰©å±•å®éªŒ

ä¿®æ”¹ `ablation_study.py` æ·»åŠ æ–°å®éªŒï¼š

```python
new_experiment = {
    'name': 'large_model',
    'd_model': 256,
    'num_heads': 8,
    'd_ff': 1024,
    'num_layers': 4
}
```

## ğŸ“ å­¦æœ¯å¼•ç”¨

å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@software{transformer_scratch_2024,
  title = {Transformer from Scratch Implementation},
  author = {Your Name},
  year = {2024},
  url = {https://github.com/your-username/transformer-from-scratch}
}
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç å’Œæå‡ºé—®é¢˜ï¼

1. Fork æœ¬é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ™ è‡´è°¢

- æ„Ÿè°¢ [Vaswani et al.](https://arxiv.org/abs/1706.03762) çš„åŸå§‹Transformerè®ºæ–‡
- æ„Ÿè°¢ PyTorch å›¢é˜Ÿæä¾›çš„ä¼˜ç§€æ¡†æ¶
- æ„Ÿè°¢å¼€æºç¤¾åŒºçš„è´¡çŒ®å’Œæ”¯æŒ

---

**æ³¨æ„**: æœ¬é¡¹ç›®ä¸ºæ•™è‚²ç›®çš„è®¾è®¡ï¼Œé€‚åˆå­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ã€‚ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ä¼˜åŒ–åº“å¦‚ [Hugging Face Transformers](https://github.com/huggingface/transformers)ã€‚